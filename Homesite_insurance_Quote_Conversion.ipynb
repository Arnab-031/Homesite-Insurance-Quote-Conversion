{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARNAB CHATTERJEE\n",
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQuVLxbU2qCH",
    "outputId": "8308b1de-95d4-4849-a704-854411e3d196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vecstack in /opt/anaconda3/lib/python3.12/site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from vecstack) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from vecstack) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/anaconda3/lib/python3.12/site-packages (from vecstack) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.18->vecstack) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.18->vecstack) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "vAUN2cYH2zyo"
   },
   "outputs": [],
   "source": [
    "from vecstack import stacking\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score #works\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter #for Smote,\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9OB3bAF5S8n",
    "outputId": "08c2963e-8b72-46c0-b6a6-9dc0f7daf62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 52738, 1: 12262})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('RevisedHomesiteTrain1.csv')\n",
    "\n",
    "# defining features and target variable\n",
    "X = data.drop(columns=['QuoteConversion_Flag'])\n",
    "y = data['QuoteConversion_Flag']\n",
    "\n",
    "# Checking the original dataset class distribution\n",
    "print(\"Original dataset shape:\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RsJK3cW5VaD",
    "outputId": "d4509d7c-d189-4423-93a7-fa2bfaf2ed20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 52738, 1: 12262})\n",
      "Resampled dataset shape: Counter({0: 52738, 1: 26369})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming X, y are your features and target respectively\n",
    "\n",
    "# Applying SMOTE to balance the dataset\n",
    "print(\"Original dataset shape:\", Counter(y))\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled dataset shape:\", Counter(y_resampled))\n",
    "\n",
    "# Spliting the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKsDfQxH59FC",
    "outputId": "8fe97817-fe37-4ce3-af71-ea0fc89f3340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "   CoverageField11A  CoverageField11B  CoverageField1A  CoverageField1B  \\\n",
      "0                 2                 1               17               23   \n",
      "1                 5                 9                6                8   \n",
      "2                 4                 6                7               12   \n",
      "3                15                23                3                2   \n",
      "4                 4                 6                8               13   \n",
      "\n",
      "   CoverageField2A  CoverageField2B  CoverageField3A  CoverageField3B  \\\n",
      "0               17               23               15               22   \n",
      "1                6                8                5                7   \n",
      "2                7               12                6               10   \n",
      "3                3                2                2                2   \n",
      "4                8               13                7               11   \n",
      "\n",
      "   CoverageField4A  CoverageField4B  ...  PropertyField38_N  \\\n",
      "0               16               22  ...                  1   \n",
      "1                5                8  ...                  1   \n",
      "2                7               11  ...                  1   \n",
      "3                3                2  ...                  1   \n",
      "4                7               13  ...                  1   \n",
      "\n",
      "   PropertyField38_Y  GeographicField63_   GeographicField63_N  \\\n",
      "0                  0                    0                    1   \n",
      "1                  0                    0                    1   \n",
      "2                  0                    0                    1   \n",
      "3                  0                    0                    1   \n",
      "4                  0                    0                    1   \n",
      "\n",
      "   GeographicField63_Y  GeographicField64_CA  GeographicField64_IL  \\\n",
      "0                    0                     1                     0   \n",
      "1                    0                     0                     0   \n",
      "2                    0                     0                     0   \n",
      "3                    0                     0                     0   \n",
      "4                    0                     0                     1   \n",
      "\n",
      "   GeographicField64_NJ  GeographicField64_TX  QuoteConversion_Flag  \n",
      "0                     0                     0                     0  \n",
      "1                     1                     0                     0  \n",
      "2                     1                     0                     0  \n",
      "3                     0                     1                     0  \n",
      "4                     0                     0                     0  \n",
      "\n",
      "[5 rows x 596 columns]\n",
      "Class distribution after applying SMOTE:\n",
      "QuoteConversion_Flag\n",
      "0    42183\n",
      "1    42183\n",
      "Name: count, dtype: int64\n",
      "Training MLP...\n",
      "Model: MLP\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90     10555\n",
      "           1       0.57      0.61      0.59      2445\n",
      "\n",
      "    accuracy                           0.84     13000\n",
      "   macro avg       0.74      0.75      0.75     13000\n",
      "weighted avg       0.84      0.84      0.84     13000\n",
      "\n",
      "AUC: 0.8642124464413206\n",
      "==================================================\n",
      "Training SVM...\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.52      0.63     10555\n",
      "           1       0.19      0.48      0.27      2445\n",
      "\n",
      "    accuracy                           0.51     13000\n",
      "   macro avg       0.50      0.50      0.45     13000\n",
      "weighted avg       0.69      0.51      0.56     13000\n",
      "\n",
      "AUC: 0.5\n",
      "==================================================\n",
      "Training DecisionTree...\n",
      "Model: DecisionTree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     10555\n",
      "           1       0.67      0.71      0.69      2445\n",
      "\n",
      "    accuracy                           0.88     13000\n",
      "   macro avg       0.80      0.82      0.81     13000\n",
      "weighted avg       0.88      0.88      0.88     13000\n",
      "\n",
      "AUC: 0.8158598595922226\n",
      "==================================================\n",
      "Training RandomForest...\n",
      "Model: RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     10555\n",
      "           1       0.78      0.64      0.71      2445\n",
      "\n",
      "    accuracy                           0.90     13000\n",
      "   macro avg       0.85      0.80      0.82     13000\n",
      "weighted avg       0.90      0.90      0.90     13000\n",
      "\n",
      "AUC: 0.9392055054883418\n",
      "==================================================\n",
      "Training KNN...\n",
      "Model: KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71     10555\n",
      "           1       0.21      0.44      0.28      2445\n",
      "\n",
      "    accuracy                           0.58     13000\n",
      "   macro avg       0.52      0.53      0.49     13000\n",
      "weighted avg       0.71      0.58      0.63     13000\n",
      "\n",
      "AUC: 0.5311189901179817\n",
      "==================================================\n",
      "Training SGDClassifier...\n",
      "Model: SGDClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.14      0.24     10555\n",
      "           1       0.21      0.98      0.34      2445\n",
      "\n",
      "    accuracy                           0.30     13000\n",
      "   macro avg       0.58      0.56      0.29     13000\n",
      "weighted avg       0.82      0.30      0.26     13000\n",
      "\n",
      "AUC: 0.5568344217018848\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 1: Loading the dataset\n",
    "train_data = pd.read_csv('RevisedHomesiteTrain1.csv')\n",
    "\n",
    "# Check if the dataset was loaded correctly or not\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Specifying the target column (adjust this if necessary)\n",
    "target_column = 'QuoteConversion_Flag'\n",
    "\n",
    "# Step 2: Separating features and target variable\n",
    "X = train_data.drop(columns=[target_column])\n",
    "y = train_data[target_column]\n",
    "\n",
    "# Step 3: Spliting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Applying SMOTE to balance the classes\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Printing class distribution after SMOTE\n",
    "print(\"Class distribution after applying SMOTE:\")\n",
    "print(y_train_smote.value_counts())\n",
    "\n",
    "# Step 5: Defining models\n",
    "models = {\n",
    "    'MLP': MLPClassifier(max_iter=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, kernel='linear', max_iter=1000, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
    "    'SGDClassifier': SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 6: Training and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Predicting and evaluate=ing\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "    # Calculating AUC if the model supports predict_proba\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "        print(\"AUC:\", roc_auc_score(y_val, y_pred_proba))\n",
    "\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'final_estimator__C': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Assuming X_train_smote_scaled and y_train_smote are already defined\n",
    "# Sample your data\n",
    "X_train_sample = pd.DataFrame(X_train_smote_scaled).sample(frac=0.5, random_state=42)\n",
    "y_train_sample = y_train_smote.loc[X_train_sample.index]\n",
    "\n",
    "# Ensure the data arrays are writeable\n",
    "X_train_sample = X_train_sample.copy()\n",
    "y_train_sample = y_train_sample.copy()\n",
    "\n",
    "# Check and print the writeable flag (optional)\n",
    "# print(\"X_train_sample writeable:\", X_train_sample.values.flags.writeable)\n",
    "# print(\"y_train_sample writeable:\", y_train_sample.values.flags.writeable)\n",
    "\n",
    "# Scale the features to improve convergence\n",
    "scaler = StandardScaler()\n",
    "X_train_sample_scaled = scaler.fit_transform(X_train_sample)\n",
    "\n",
    "# Define base estimators\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('svc', SVC(probability=True, max_iter=1000)),\n",
    "    ('mlp', MLPClassifier(max_iter=1000))\n",
    "]\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=1  # Use a single core to avoid parallel processing issues\n",
    ")\n",
    "\n",
    "# Define parameter distributions for random search\n",
    "param_dist = {\n",
    "    'final_estimator__C': np.logspace(-4, 4, 10)\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=stacking_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    n_jobs=1  # Use a single core to avoid parallel processing issues\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_sample_scaled, y_train_sample)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Evaluating Base Models:\n",
      "Model: rf, Accuracy: 0.8350\n",
      "Model: svm, Accuracy: 0.8400\n",
      "Model: mlp, Accuracy: 0.8700\n",
      "Model: sgd, Accuracy: 0.8650\n",
      "Model: dt, Accuracy: 0.8150\n",
      "Model: knn, Accuracy: 0.5500\n",
      "\n",
      "Evaluating Stacking Model (Best Model):\n",
      "Best Model Accuracy: 0.8250\n",
      "Best Model AUC: 0.3984\n",
      "\n",
      "Classification Report for Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       165\n",
      "           1       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.41      0.50      0.45       200\n",
      "weighted avg       0.68      0.82      0.75       200\n",
      "\n",
      "\n",
      "Best Hyperparameters: {'final_estimator__n_estimators': 100, 'final_estimator__min_samples_split': 5, 'final_estimator__min_samples_leaf': 2, 'final_estimator__max_depth': 10, 'final_estimator__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Step 1: Loading the dataset (sample 1000 rows for fast execution)\n",
    "train_data = pd.read_csv('RevisedHomesiteTrain1.csv').sample(n=1000, random_state=42)\n",
    "target_column = 'QuoteConversion_Flag'\n",
    "\n",
    "# Step 2: Separating features and target variable\n",
    "X = train_data.drop(columns=[target_column])\n",
    "y = train_data[target_column]\n",
    "\n",
    "# Step 3: Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Applying SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 5: Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_smote_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Step 6: Defining base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42, max_iter=2000)),  # Increased max_iter\n",
    "    ('mlp', MLPClassifier(max_iter=200, random_state=42, batch_size=64)),  # Modified parameters for MLP\n",
    "    ('sgd', SGDClassifier(loss='log_loss', max_iter=500, random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "]\n",
    "\n",
    "# Step 7: Defining stacking model\n",
    "final_estimator = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=final_estimator, n_jobs=1)  # Set n_jobs to 1\n",
    "\n",
    "# Step 8: Defining hyperparameter grid\n",
    "param_grid = {\n",
    "    'final_estimator__n_estimators': [50, 100],\n",
    "    'final_estimator__max_depth': [None, 10],\n",
    "    'final_estimator__min_samples_split': [2, 5],\n",
    "    'final_estimator__min_samples_leaf': [1, 2],\n",
    "    'final_estimator__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Step 9: Hyperparameter tuning with RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=stacking_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=5,  # Limiting the combinations for faster execution\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,  # Set to 1 to avoid parallel errors\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Training with a subset of data for faster execution\n",
    "X_train_sample = pd.DataFrame(X_train_smote_scaled).sample(frac=0.5, random_state=42).to_numpy()\n",
    "X_train_sample = np.array(X_train_sample, copy=True, order='C')  # Convert to writable array\n",
    "y_train_sample = y_train_smote.loc[X_train_sample[:, 0].argsort()[:len(X_train_sample)]]\n",
    "\n",
    "random_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Step 10: Evaluating all models\n",
    "print(\"Evaluating Base Models:\")\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train_smote_scaled, y_train_smote)  # Train base models\n",
    "    y_val_pred_base = model.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred_base)\n",
    "    print(f\"Model: {name}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 11: Evaluating best model from hyperparameter tuning\n",
    "print(\"\\nEvaluating Stacking Model (Best Model):\")\n",
    "best_model = random_search.best_estimator_\n",
    "y_val_pred_best = best_model.predict(X_val_scaled)\n",
    "best_model_accuracy = accuracy_score(y_val, y_val_pred_best)\n",
    "print(f\"Best Model Accuracy: {best_model_accuracy:.4f}\")\n",
    "\n",
    "# Calculating AUC for best model\n",
    "if hasattr(best_model, 'predict_proba'):\n",
    "    y_val_proba_best = best_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    best_model_auc = roc_auc_score(y_val, y_val_proba_best)\n",
    "    print(f\"Best Model AUC: {best_model_auc:.4f}\")\n",
    "\n",
    "# Step 12: Final classification report\n",
    "print(\"\\nClassification Report for Best Model:\")\n",
    "print(classification_report(y_val, y_val_pred_best))\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'result.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Saving model predictions\n",
    "# Ensuring the test dataset has the same columns as the training data\n",
    "test_data = pd.read_csv('RevisedHomesiteTest1.csv')  # Assuming test data file name\n",
    "test_data_aligned = test_data[X.columns]\n",
    "\n",
    "# Checking if any columns are missing after alignment\n",
    "missing_cols = set(X.columns) - set(test_data_aligned.columns)\n",
    "if missing_cols:\n",
    "    print(f\"Warning: The following columns are missing in the test data: {missing_cols}\")\n",
    "\n",
    "# Making predictions using the aligned test dataset\n",
    "test_predictions = best_model.predict_proba(test_data_aligned)[:, 1]\n",
    "\n",
    "# Creating a submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'QuoteNumber': test_data['QuoteNumber'],\n",
    "    'QuoteConversion_Flag': test_predictions\n",
    "})\n",
    "\n",
    "# Saving the submission to CSV\n",
    "submission_file = 'result.csv'\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"Submission file saved as '{submission_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
